{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random as ran\n",
    "\n",
    "###Probably want to initialise the network once. This is supposed to be done with a textfile. \n",
    "\n",
    "class network():\n",
    "    def __init__(self, total_layers, pixel_n, minibatch_size, loss_func, lrate = 0.1, fileread = False, file = None):\n",
    "        self.file = file\n",
    "        self.depth = total_layers\n",
    "        self.lrate = lrate\n",
    "        self.lfunc = loss_func\n",
    "        self.layers = [None] * total_layers\n",
    "\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                self.layers[i] = layer(None, size = (pixel_n**2, minibatch_size), activ_func = \"nothing\")\n",
    "            elif i == total_layers - 1:\n",
    "                self.layers[i] = layer(self.layers[i-1], size = (10, minibatch_size), activ_func = \"softmax\")\n",
    "            else: \n",
    "                self.layers[i] = layer(self.layers[i-1], size = (10, minibatch_size), activ_func = \"basic\")\n",
    "        for layer in range(total_layers - 1):\n",
    "            (self.layers[layer]).setChild(self.layers[layer + 1])\n",
    "\n",
    "\n",
    "\n",
    "    def getDepth(self):\n",
    "        return self.depth\n",
    "    def uppdateParameters(self, deltaW, deltaBias): #delta W is array of numpy W matrises\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            layer.set_W(layer.getW() + self.lrate*deltaW[i])\n",
    "            layer.setBias(layer.getBias() + self.lrate*deltaBias[i])\n",
    "    def fetch_active(string):\n",
    "        return \n",
    "\n",
    "\n",
    "class layer():\n",
    "    def __init__(self, parent, size, activ_func, bias = False, child = None):\n",
    "        self.parent = parent\n",
    "        self.child = child\n",
    "        self.size = size\n",
    "        self.activ_func = activ_func\n",
    "        if not bias:\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.bias = np.ones(size)\n",
    "        self.W = np.ones((size[0], parent.get_size())) # n x m\n",
    "        self.X = np.zeros((size)) # n x m @ m x C\n",
    "\n",
    "    def getSize(self):\n",
    "        return self.size\n",
    "    def getX(self):\n",
    "        return self.X\n",
    "    def getW(self):\n",
    "        return self.W\n",
    "    def getActiv_func(self):\n",
    "        return self.activ_func\n",
    "    def getBias(self):\n",
    "        return self.bias\n",
    "\n",
    "    def setW(self, W):\n",
    "        self.W = W\n",
    "    def setBias(self, b):\n",
    "        self.W = b\n",
    "    def setChild(self, child):\n",
    "        self.child = child\n",
    "    def setX(self, X):\n",
    "        self.X = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd059e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random as ran\n",
    "\n",
    "###This should set the inputlayer of the network as the minibatch C, and then propegate by F(W@C + bias) one layer at the time\n",
    "### NB: Only X is changed ever in this function. The X of the last layer shoult be the output of the network\n",
    "def propagate_forward(network, minibatch):\n",
    "    network.layers[0].setX = minibatch\n",
    "    for count, layer in enumerate(network.layers):\n",
    "        if count == 0:\n",
    "            continue\n",
    "        else:\n",
    "            layer.setX(layer.getActiv_func(layer.getW() @ (layer.getparent()).getX() + layer.getBias()))\n",
    "        \n",
    "    return network\n",
    "\n",
    "###Assuming forward propagation has been done, the network is not inputted into the backpropagation funciton. The network should\n",
    "### now be filled with values on the nodes. In this algorithm, a deltaW\n",
    "### with dim = sum(dimWi) = sum(#nodes_layer_i * #nodes_layer_i-1) should be returned. \n",
    "### Not yet sure if I should delete all Xs in the network after each backpropagation, feels a bit like not whiping after taking a\n",
    "\n",
    "def propagate_backward(network):\n",
    "    deltaW = np.zeros(network.getDepth() - 1)\n",
    "    network.lfunc = network.loss_func\n",
    "    # d lfunc/d Wi = d lfunc/d xN * d Xn/d xN-1 * ... * d xi+1/d xi * d xi/d activfunc * d activefunc/d(Wixi + Bi) * xi\n",
    "    #  which should be gradient * matrix * ... * matrix, and each of these should be evaluated on the value in the denominator\n",
    "    # of its corresponding derivative. This must be as in the chain rule, df(x^2+1)/dx = df/du*du/dx, in if it shall be\n",
    "    # evaluated anyway, and you already know the value of u = x^2+1, just evaluate df/du at u and du/dx at x.\n",
    "    return deltaW\n",
    "\n",
    "\n",
    "### Picks out random minibatches and trains(forward + backward prop) for the given amount of epochs, with uppdating of weighthappening\n",
    "### only after each epoch.\n",
    "def train_network_with_SGD(network, S, C, epochs = 10):#S = training set, C = size of minibatch\n",
    "    deltaW = np.zeros(network.getDepth() - 1)\n",
    "    epoch = len(S)/len(C)\n",
    "    for i in range(epochs):\n",
    "        for j in range(epoch):\n",
    "            minibatch = ran.sample(S, C)\n",
    "            network_copy = deepcopy(network) #seems slow, is it really nescesary?\n",
    "            propagate_forward(network_copy, minibatch) #fills the network with values for X\n",
    "            deltaW += propagate_backward(network_copy) #returns delta W's for minibatch C\n",
    "        network.uppdateW(deltaW) #uppdate after every epoch\n",
    "    return network\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return max(x,0)\n",
    "\n",
    "def DReLU(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def ReLU(x):\n",
    "    return max(x,0)\n",
    "\n",
    "def DReLU(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "def Dsigmoid(x):\n",
    "    z = sigmoid(x)\n",
    "    return z * (1 - z)\n",
    "\n",
    "def SE(x):\n",
    "    return np.norm(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3a0235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jupyterthemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4fff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t onedork"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
